{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "load_dotenv(find_dotenv())\n",
    "import requests\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "from haystack.pipelines import Pipeline\n",
    "from haystack.nodes import PreProcessor\n",
    "from haystack.document_stores import ElasticsearchDocumentStore\n",
    "from haystack.utils import print_answers\n",
    "from haystack.nodes import EmbeddingRetriever, DensePassageRetriever, MultihopEmbeddingRetriever, BM25Retriever, JoinDocuments, SentenceTransformersRanker\n",
    "from haystack.nodes import TransformersQueryClassifier\n",
    "from qdrant_haystack.document_stores import QdrantDocumentStore\n",
    "from haystack.schema import Document\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "import time\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import xmltodict\n",
    "import gradio as gr\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot():\n",
    "    def __init__(self):\n",
    "        self.preprocessor = PreProcessor(\n",
    "        clean_empty_lines=True,\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=True,\n",
    "        split_by=\"word\",\n",
    "        split_length=200,\n",
    "        split_respect_sentence_boundary=True,\n",
    "        split_overlap=10\n",
    "        )       \n",
    "\n",
    "        self.query_classifier = TransformersQueryClassifier()\n",
    "\n",
    "        self.join_documents = JoinDocuments(\"concatenate\")\n",
    "\n",
    "        self.ranker = SentenceTransformersRanker(model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "        self.doc_params = {\"Ranker\": {\"top_k\":4}, \"OpenAIRetriever\": {\"top_k\":20}, \"DPRRetriever\": {\"top_k\":20}, \"MultihopRetriever\": {\"top_k\":20}, \"BM25Retriever\": {\"top_k\":20}}\n",
    "        self.context_delimiter = \"####\"\n",
    "        self.query_delimiter = \"````\"\n",
    "        self.HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'}\n",
    "\n",
    "        self.full_chat_history = []\n",
    "        self.current_chat_history = []\n",
    "        self.default_message = {\"role\": \"system\", \"content\": \"You are a helpful and firendly assistant\"}\n",
    "        self.messages = [self.default_message]\n",
    "        self.len_chat_history = 4\n",
    "        self.role = \"You are a helpful and firendly assistant\"\n",
    "\n",
    "        self.model = \"gpt-3.5-turbo\"\n",
    "        self.temperature = 0\n",
    "    \n",
    "    def set_doc_params(self,doc_params):\n",
    "        self.doc_params = doc_params\n",
    "\n",
    "    def set_role(self,role):\n",
    "        self.role = role\n",
    "\n",
    "    def set_model(self,model):\n",
    "        self.model = model\n",
    "\n",
    "    def set_temperature(self,temperature):\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def extract_text_from_url(self,url, headers = None):\n",
    "                if headers is None:\n",
    "                    headers = self.HEADERS\n",
    "                html = requests.get(url,headers=headers).text\n",
    "                soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "                text = soup.get_text()\n",
    "                lines = (line.strip() for line in text.splitlines())\n",
    "                return '\\n'.join(line for line in lines if line)\n",
    "\n",
    "    def get_urls(self,sitemap_url, headers = None):\n",
    "                urls = []\n",
    "                if headers is None:\n",
    "                    headers = self.HEADERS\n",
    "                sitemap = requests.get(sitemap_url,headers=headers).text\n",
    "                try:\n",
    "                    sitemap = xmltodict.parse(sitemap)\n",
    "                    if 'sitemapindex' in sitemap:\n",
    "                        sitemap = sitemap['sitemapindex']['sitemap']\n",
    "                        for entry in sitemap:\n",
    "                            urls += self.get_urls(entry['loc'])\n",
    "                    else:\n",
    "                        sitemap = sitemap['urlset']['url']\n",
    "                        for entry in sitemap:\n",
    "                            urls.append(entry['loc'])\n",
    "                except:\n",
    "                    print(f\"Error parsing sitemap {sitemap_url}\")\n",
    "                return urls\n",
    "\n",
    "    def get_pages(self,urls):\n",
    "                pages = []\n",
    "                for url in urls:\n",
    "                    try:\n",
    "                        pages.append({'text': self.extract_text_from_url(url), 'source': url})\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                return pages\n",
    "\n",
    "    def get_documents(self,pages):\n",
    "        docs = []\n",
    "        for page in pages:\n",
    "            document = Document(content=page['text'], meta={\"url\": page['source']},content_type=\"text\")\n",
    "            docs.append(document)\n",
    "        docs = self.preprocessor.process(docs)\n",
    "        return docs\n",
    "    \n",
    "    def make_document_stores(self,index, docs ,path = \"src/data/qdrant\"):\n",
    "        self.bm25_document_store = ElasticsearchDocumentStore(\n",
    "        # path = path,\n",
    "        index=f\"bm25_{index}\",\n",
    "        recreate_index=True,\n",
    "        )\n",
    "\n",
    "        self.openai_document_store = ElasticsearchDocumentStore(\n",
    "            # path = path,\n",
    "            index=f\"openai_{index}\",\n",
    "            embedding_dim=1536,\n",
    "            recreate_index=True,\n",
    "        )\n",
    "        self.multihop_document_store = ElasticsearchDocumentStore(\n",
    "            # path = path,\n",
    "            index=f\"multihop_{index}\",\n",
    "            embedding_dim=384,\n",
    "            recreate_index=True,\n",
    "        )\n",
    "        self.dpr_document_store = ElasticsearchDocumentStore(\n",
    "            # path = path,\n",
    "            index=f\"dpr_{index}\",\n",
    "            embedding_dim=768,\n",
    "            recreate_index=True,\n",
    "            similarity=\"dot_product\"\n",
    "        )\n",
    "\n",
    "        self.get_retrievers()\n",
    "\n",
    "        self.bm25_document_store.write_documents(docs)\n",
    "\n",
    "        self.openai_document_store.write_documents(docs)\n",
    "        self.openai_document_store.update_embeddings(self.openai_retriever)\n",
    "\n",
    "        self.multihop_document_store.write_documents(docs)\n",
    "        self.multihop_document_store.update_embeddings(self.multihop_retriever)\n",
    "\n",
    "        self.dpr_document_store.write_documents(docs)\n",
    "        self.dpr_document_store.update_embeddings(self.dpr_retriever)\n",
    "    \n",
    "    def make_document_stores_from_sitemap(self,index,sitemap_url):\n",
    "        urls = self.get_urls(sitemap_url)\n",
    "        pages = self.get_pages(urls)\n",
    "        docs = self.get_documents(pages)\n",
    "        self.make_document_stores(index,docs)\n",
    "\n",
    "    def get_document_stores(self,index,path = \"src/data/qdrant\"):\n",
    "        self.bm25_document_store = ElasticsearchDocumentStore(\n",
    "            # path = path,\n",
    "            index=f\"bm25_{index}\"\n",
    "        )\n",
    "\n",
    "        self.openai_document_store = ElasticsearchDocumentStore(\n",
    "            # path = path,\n",
    "            index=f\"openai_{index}\",\n",
    "            embedding_dim=1536\n",
    "        )\n",
    "        self.multihop_document_store = ElasticsearchDocumentStore(\n",
    "            # path = path,\n",
    "            index=f\"multihop_{index}\",\n",
    "            embedding_dim=384\n",
    "        )\n",
    "        self.dpr_document_store = ElasticsearchDocumentStore(\n",
    "            # path = path,\n",
    "            index=f\"dpr_{index}\",\n",
    "            embedding_dim=768,\n",
    "            similarity=\"dot_product\"\n",
    "        )\n",
    "\n",
    "        self.get_retrievers()\n",
    "    \n",
    "    def get_retrievers(self):\n",
    "        self.bm25_retriever = BM25Retriever(document_store=self.bm25_document_store)\n",
    "\n",
    "        self.openai_retriever = EmbeddingRetriever(\n",
    "        document_store=self.openai_document_store,\n",
    "        batch_size=8,\n",
    "        embedding_model=\"text-embedding-ada-002\",\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        max_seq_len=1536,progress_bar=False\n",
    "        )\n",
    "\n",
    "        self.multihop_retriever = MultihopEmbeddingRetriever(\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            document_store=self.multihop_document_store,model_format='sentence_transformers',progress_bar=False)\n",
    "\n",
    "        self.dpr_retriever = DensePassageRetriever(\n",
    "            document_store=self.dpr_document_store,\n",
    "            query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "            passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "            progress_bar=False\n",
    "        )\n",
    "\n",
    "    def make_document_pipeline(self):\n",
    "        pipe = Pipeline()\n",
    "        pipe.add_node(component=self.openai_retriever, name=\"OpenAIRetriever\", inputs=[\"Query\"])\n",
    "        pipe.add_node(component=self.dpr_retriever, name=\"DPRRetriever\", inputs=[\"Query\"])\n",
    "        pipe.add_node(component=self.multihop_retriever, name=\"MultihopRetriever\", inputs=[\"Query\"])\n",
    "        pipe.add_node(component=self.bm25_retriever, name=\"BM25Retriever\", inputs=[\"Query\"])\n",
    "        pipe.add_node(component=self.join_documents, name=\"JoinDocuments\", inputs=[\"OpenAIRetriever\",\"DPRRetriever\",\"MultihopRetriever\",\"BM25Retriever\"])\n",
    "        pipe.add_node(component=self.ranker, name=\"Ranker\", inputs=[\"JoinDocuments\"])\n",
    "\n",
    "        self.document_pipeline = pipe\n",
    "\n",
    "    def get_context(self,query,pipe = None, return_sources= False,*args,**kwargs):\n",
    "        if pipe is None: pipe = self.document_pipeline\n",
    "        if kwargs is None: kwargs = {\"params\":self.doc_params}\n",
    "        docs = pipe.run(query,*args, **kwargs)\n",
    "        sources = []\n",
    "        context_string = \"\"\n",
    "        for document in docs['documents']:\n",
    "            context_string += document.content + '\\n' \n",
    "            sources.append(document.meta['url'])\n",
    "        if return_sources:\n",
    "            return (context_string,sources)\n",
    "        else:\n",
    "            return context_string\n",
    "        \n",
    "    def get_conversation(self):\n",
    "        conversation = \"\"\n",
    "        for i in range(0,len(self.current_chat_history),2):\n",
    "            conversation += \"Human: \"+self.current_chat_history[i]['content'] + '\\n'\n",
    "            conversation += \"AI: \"+self.current_chat_history[i+1]['content'] + '\\n'\n",
    "        return conversation\n",
    "        \n",
    "    def get_response(self,query, model = None,temperature = None,return_sources = False, debug = False):\n",
    "        if model is None: model = self.model\n",
    "        if temperature is None: temperature = self.temperature\n",
    "        conversation = self.get_conversation()\n",
    "        \n",
    "        check_relation_prompt = f\"\"\"\n",
    "            You have been given a conversation history between a human and an AI which is enclosed by {self.context_delimiter} and the most recent query\n",
    "            of the human which is enclosed by {self.query_delimiter}. Your job is to determine if the most recent query of the human refers to some information\n",
    "            or context present in the conversation history or if it is a standalone query which is not related to the conversation history.\n",
    "            Respond with a Y or N character, with no punctuation:\n",
    "            Y - If the query refers to something in the conversation history or is related to the conversation history\n",
    "            N - otherwise\n",
    "\n",
    "            Output a single letter only.\n",
    "\n",
    "            Context:\n",
    "            {self.context_delimiter} {conversation} {self.context_delimiter}\n",
    "\n",
    "            Query:\n",
    "            {self.query_delimiter} {query} {self.query_delimiter}\n",
    "            \"\"\"\n",
    "        check_relation_message=[{\"role\": \"user\", \"content\": check_relation_prompt}]\n",
    "        completion = openai.ChatCompletion.create(\n",
    "                        model=model,\n",
    "                        messages=check_relation_message,\n",
    "                        temperature=temperature,\n",
    "                        max_tokens=1\n",
    "                        )\n",
    "        related = completion.choices[0]['message']['content'].strip().lower()\n",
    "        \n",
    "        if debug:\n",
    "            pprint(related)\n",
    "        \n",
    "        if related == \"y\":\n",
    "            condense_question_prompt = f'''\n",
    "                You are given a part of a conversation history between a human and an AI which is enclosed by {self.context_delimiter} and the most recent query \n",
    "                of the human which is enclosed by {self.query_delimiter}.\n",
    "                If the query is not related to the conversation history then do not modify the query and return the query as is.\n",
    "                Otherwise your role is to form a standalone question using the conversation history and the query of the human which can then be answered by an AI \n",
    "                without needing to know the conversation history.\n",
    "\n",
    "                Context: \n",
    "                {self.context_delimiter} {conversation} {self.context_delimiter}\n",
    "                \n",
    "                Query: \n",
    "                {self.query_delimiter} {query} {self.query_delimiter}\n",
    "\n",
    "                Standalone Question:\n",
    "                '''\n",
    "            condense_question_message=[{\"role\": \"user\", \"content\": condense_question_prompt}]\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                            model=model,\n",
    "                            messages=condense_question_message,\n",
    "                            temperature=temperature,\n",
    "                            )\n",
    "            modified_query = completion.choices[0]['message']['content']\n",
    "        else:\n",
    "            modified_query = query\n",
    "\n",
    "        if debug:\n",
    "            pprint(modified_query)\n",
    "        context,sources = self.get_context(modified_query,self.document_pipeline,return_sources=True,params = self.doc_params)\n",
    "        if debug:\n",
    "            pprint(context)\n",
    "            pprint(sources)\n",
    "        system_prompt = f'''\n",
    "            You are a {self.role}.\n",
    "            You are given some context which is enclosed by {self.context_delimiter} and a query which is enclosed by {self.query_delimiter}.\n",
    "            Your role is to assist customers by providing accurate information, offering helpful recommendations, and guiding them towards the solutions of their issues. \n",
    "            Feel free to ask clarifying questions only if needed, to better understand the customer's needs and preferences. \n",
    "            Leverage the provided context and information in the question itself to answer the question effectively without generating false or fictional information. \n",
    "            Double check your response for accuracy. Your responses should be short, friendly and humanlike.\n",
    "            Respond only to the following question using only the context and the information given in the question.\n",
    "            Only use your existing knowledge for generic information and not for specific information. Do not make up any figures or facts.\n",
    "            If you don't know the answer respond with \"May I connect you with an expert in this topic to discuss this in detail?\":\n",
    "\n",
    "            Context: \n",
    "            {self.context_delimiter} {context} {self.context_delimiter}\n",
    "\n",
    "            Remember, your expertise and helpfulness are key in assisting customers in making informed choices.'''\n",
    "            \n",
    "        self.messages[0]={\"role\": \"system\", \"content\": system_prompt}\n",
    "        self.messages.append({\"role\": \"user\", \"content\": f\"{self.query_delimiter}{modified_query}{self.query_delimiter}\"})\n",
    "        completion = openai.ChatCompletion.create(\n",
    "                        model=model,\n",
    "                        messages=self.messages,\n",
    "                        temperature=temperature,\n",
    "                        )\n",
    "        if debug:\n",
    "            pprint(completion)\n",
    "        response = completion.choices[0]['message']['content']\n",
    "        self.full_chat_history.append({\"role\": \"user\", \"content\": f\"{query}\"})\n",
    "        self.full_chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        if len(self.current_chat_history) == 2*self.len_chat_history:\n",
    "            self.current_chat_history = self.current_chat_history[2:]\n",
    "        self.current_chat_history.append({\"role\": \"user\", \"content\": f\"{query}\"})\n",
    "        self.current_chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        self.messages = [self.messages[0]] + self.current_chat_history\n",
    "        if return_sources:\n",
    "            return (response,sources)\n",
    "        else:\n",
    "            return response\n",
    "\n",
    "    def clear_chat(self):\n",
    "        self.current_chat_history.clear()\n",
    "        self.messages = [self.default_message]\n",
    "        self.full_chat_history = []\n",
    "\n",
    "    def launch(self,share = False,show_sources= False, debug = False):\n",
    "        with gr.Blocks() as demo:\n",
    "            chatbot = gr.Chatbot()\n",
    "            msg = gr.Textbox()\n",
    "            clear = gr.Button(\"Clear\")            \n",
    "            def user(user_message, history):\n",
    "                return \"\", history + [[user_message, None]]\n",
    "            def bot(history):\n",
    "                response,sources = self.get_response(history[-1][0],debug = debug,return_sources=True)\n",
    "                urls = \"\"\n",
    "                for url in sources:\n",
    "                    urls += url\n",
    "                    urls += \"\\n\"\n",
    "                if show_sources:\n",
    "                    bot_message = response + \"\\n\" + urls\n",
    "                else: \n",
    "                    bot_message = response\n",
    "                history[-1][1] = \"\"\n",
    "                for character in bot_message:\n",
    "                    history[-1][1] += character\n",
    "                    time.sleep(0.01)\n",
    "                    yield history\n",
    "\n",
    "            msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "                bot, chatbot, chatbot\n",
    "            )\n",
    "            clear.click(self.clear_chat,  None, chatbot, queue=False)\n",
    "        demo.queue()\n",
    "        demo.launch(share=share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "chatbot = Chatbot()\n",
    "# chatbot.make_document_stores_from_sitemap(\"sbnri\",\"https://sbnri.com/sitemap.xml\")\n",
    "chatbot.get_document_stores(\"sbnri\")\n",
    "chatbot.make_document_pipeline()\n",
    "role = \"customer support guide representing SBNRI, a reputable online platform known for solving the banking needs of NRI's in India\"\n",
    "chatbot.set_role(role)\n",
    "chatbot.set_temperature(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/.local/lib/python3.10/site-packages/gradio/networking.py:57: ResourceWarning: unclosed <socket.socket fd=169, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('0.0.0.0', 0)>\n",
      "  s = socket.socket()  # create a socket object\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/shubham/.local/lib/python3.10/site-packages/gradio/networking.py:57: ResourceWarning: unclosed <socket.socket fd=171, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('0.0.0.0', 0)>\n",
      "  s = socket.socket()  # create a socket object\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/shubham/.local/lib/python3.10/site-packages/gradio/networking.py:57: ResourceWarning: unclosed <socket.socket fd=172, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('0.0.0.0', 0)>\n",
      "  s = socket.socket()  # create a socket object\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chatbot.launch(show_sources=True,debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack.nodes.retriever import EmbeddingRetriever\n",
    "# from haystack.document_stores import InMemoryDocumentStore\n",
    "# from haystack.nodes.question_generator.question_generator import QuestionGenerator\n",
    "# from haystack.nodes.label_generator.pseudo_label_generator import PseudoLabelGenerator\n",
    "\n",
    "# document_store = InMemoryDocumentStore()\n",
    "# document_store.write_documents(chatbot.bm25_document_store.get_all_documents())\n",
    "\n",
    "# retriever = EmbeddingRetriever(document_store=document_store, \n",
    "#                                embedding_model=\"sentence-transformers/msmarco-distilbert-base-tas-b\", \n",
    "#                                model_format=\"sentence_transformers\",\n",
    "#                                max_seq_len=200)\n",
    "# document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qg = QuestionGenerator(model_name_or_path=\"doc2query/msmarco-t5-base-v1\", max_length=64, split_length=200, batch_size=12)\n",
    "# psg = PseudoLabelGenerator(qg, retriever)\n",
    "# output, _ = psg.run(documents=document_store.get_all_documents()) \n",
    "# retriever.train(output[\"gpl_labels\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
